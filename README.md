# ğŸ“ˆ ETL Pipeline for Stock Market ğŸ“‰

An end-to-end ETL (Extract, Transform, Load) pipeline designed to process and analyze stock market data using **PySpark** for distributed data processing. 
This project enables data ingestion, transformation, and query-ready storage, structured in **Staging**, **Silver**, and **Gold** layers using PySpark API

## ğŸ“‘ Table of Contents

- [ğŸ“‹ Project Overview](#project-overview)
- [ğŸ—ï¸ Architecture](#architecture)
- [âœ¨ Features](#features)
- [ğŸ’» Technologies Used](#technologies-used)
- [ğŸ“Š Data Sources](#data-sources)
- [ğŸš€ Usage](#usage)
- [ğŸŒŸ Future Enhancements](#future-enhancements)
- [ğŸ¤ Contributing](#contributing)

---

## ğŸ“‹ Project Overview

This project gives a demo of the data ingestion, processing, and transformation of stock market data. 
It enables data collection from various sources, cleansing, transformation, and storage in a format ready for analytical querying. 

### Project Highlights:
- **Staging Layer**: ğŸ—ƒï¸ Ingests and stores raw data.
- **Silver Layer**: ğŸ› ï¸ Cleanses and joins data to get some historical insights data.
- **Gold Layer**: ğŸ† Aggregates and enriches data for analytical use.

## ğŸ—ï¸ Architecture

The architecture follows a **Medallion** approach with three layers.

<div align="center">
<img src="https://github.com/hrishikesh-2000/ETL-Pipeline-For-Stock-Market/blob/Initial_dev/schema/Stock%20Market%20Schema.png" alt="ETL Pipeline Diagram" width="500"/>
</div>

## âœ¨ Features

- **ğŸ”Œ Data Ingestion**: Extraction of stock data from CSVs and APIs.
- **ğŸ§¼ Data Transformation**: Data cleaning and transformation using PySpark.
- **ğŸ” SQL Compatibility**: Creates global temporary views for SQL querying.
- **ğŸ“… Weekly & Monthly Summaries**: Aggregates and calculates metrics like SMA and returns.

## ğŸ’» Technologies Used

- **Python** ğŸ
- **PySpark** âš¡ - for distributed data processing
- **Pandas** ğŸ¼ - for data handling
- **Upstox API** ğŸ”— - for stock market data
- **CSV** ğŸ“„ - for metadata and static data files

## ğŸ“Š Data Sources

- **Upstox API**: Fetches real-time and historical stock data ğŸ“ˆ.
- **NSE CSV Files**: Provides metadata for stock information ğŸ“ƒ.

## ğŸš€ Usage
- Run Staging.py Script: ğŸ—ƒï¸ Loads raw data into staging tables.
- Run Silver.py Script: ğŸ”„ Cleans and integrates the data.
- Run main.py Script: ğŸ“Š Creates weekly and monthly summaries for analysis.

## ğŸŒŸ Future Enhancements
- ğŸ”„ Automated Scheduler: Integrate with Apache Airflow for scheduled runs.
- ğŸ” Data Validation: Implement data checks at each stage.
- âš¡ Real-Time Data Integration: Add streaming data capabilities.
- ğŸ“Š Interactive Dashboard: Visualize data using Tableau or Power BI.
- âš ï¸ Error Handling & Logging

## ğŸ¤ Contributing
Contributions and feedback are welcome! ğŸ‰ Follow the steps below to contribute:

- Fork the repository ğŸ´.
- Create a new branch (git checkout -b feature-branch).
- Make your changes and commit ğŸ“.
- Push to the branch (git push origin feature-branch).
- Open a pull request ğŸš€.
---

### Prerequisites

- Python 2.9 ğŸ
- Apache Spark and PySpark âš¡
- Upstox API account ğŸ”‘

### Setup Instructions

1. **Clone the Repository**
2. Install Dependencies: Use requirements.txt for easy package installation
3. Spark Configuration: Ensure the spark_config.py script initiates a Spark session
4. Prepare Data Files: Place necessary CSV files in the data/ directory

